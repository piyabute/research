{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------------\n",
    "# 1. LIBRARY\n",
    "# -------------------------------------------------------------------------------------\n",
    "\n",
    "# https://numpy.org/devdocs/user/absolute_beginneencoder_rs.html\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors\n",
    "# https://matplotlib.org/stable/gallery/mplot3d/scatter3d.html\n",
    "# from mpl_toolkits.mplot3d import Axes3D\n",
    "import os\n",
    "num_cores = os.cpu_count()\n",
    "import glob\n",
    "# File processing\n",
    "import sys\n",
    "# import scipy.spatial.distance as sc\n",
    "\n",
    "# KDTree\n",
    "# from sklearn.neighbors import KDTree\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "# Distance Calculation\n",
    "import math, statistics\n",
    "from scipy.spatial import distance\n",
    "\n",
    "# from chebyshev import Chebyshev\n",
    "import numpy.polynomial.chebyshev \n",
    "\n",
    "# Data Transformation\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, MinMaxScaler, normalize\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Baselines (Undersampling)\n",
    "# https://imbalanced-learn.org/\n",
    "from imblearn.under_sampling import AllKNN\n",
    "from imblearn.under_sampling import ClusterCentroids \n",
    "from imblearn.under_sampling import CondensedNearestNeighbour\n",
    "from imblearn.under_sampling import EditedNearestNeighbours\n",
    "from imblearn.under_sampling import RepeatedEditedNearestNeighbours\n",
    "from imblearn.under_sampling import InstanceHardnessThreshold\n",
    "from imblearn.under_sampling import NearMiss \n",
    "from imblearn.under_sampling import NeighbourhoodCleaningRule \n",
    "from imblearn.under_sampling import OneSidedSelection\n",
    "from imblearn.under_sampling import RandomUnderSampler \n",
    "from imblearn.under_sampling import TomekLinks \n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from collections import Counter\n",
    "\n",
    "# Model\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import StratifiedKFold # train_test_split, GridSearchCV, cross_validate, cross_val_score, StratifiedKFold\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold # train_test_split, GridSearchCV, cross_validate, cross_val_score, StratifiedKFold\n",
    "from sklearn.model_selection import RepeatedKFold # train_test_split, GridSearchCV, cross_validate, cross_val_score, StratifiedKFold\n",
    "from sklearn.model_selection import KFold # train_test_split, GridSearchCV, cross_validate, cross_val_score, StratifiedKFold\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Performance\n",
    "# from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, cohen_kappa_score\n",
    "from sklearn.metrics import make_scorer, cohen_kappa_score\n",
    "\n",
    "# Complexity\n",
    "import time\n",
    "import tracemalloc\n",
    "\n",
    "# Multiprocessing\n",
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "from pathlib import Path\n",
    "\n",
    "# Warning\n",
    "import warnings\n",
    "\n",
    "# -------------------------------------------------------------------------------------\n",
    "# 2. FUNCTIONS\n",
    "# -------------------------------------------------------------------------------------\n",
    "\n",
    "def custom_distance(x, y):\n",
    "    if x[-1] == y[-1]:\n",
    "        return np.inf\n",
    "    else:\n",
    "        return np.linalg.norm(x[:-1] - y[:-1])\n",
    "\n",
    "def find_minimum_distance(d,i): # https://docs.scipy.org/doc/scipy/reference/spatial.distance.html\n",
    "    min_dist = 999999999999999.0\n",
    "    min_vec = -1\n",
    "    if (d == 0): # Cityblock\n",
    "        for j in range(tempdata.shape[0]):\n",
    "            if (templabel[i] != templabel[j]):\n",
    "#                new_dist = distance.cityblock(tempdata[i],tempdata[j])\n",
    "                new_dist = distance.minkowski(tempdata[i],tempdata[j],1)\n",
    "                if (new_dist < min_dist):\n",
    "                    min_dist = new_dist\n",
    "                    min_vec = j\n",
    "    elif (d == 1): # Chebyshev\n",
    "        for j in range(tempdata.shape[0]):\n",
    "            if (templabel[i] != templabel[j]):\n",
    "                new_dist = distance.chebyshev(tempdata[i],tempdata[j])\n",
    "                if (new_dist < min_dist):\n",
    "                    min_dist = new_dist\n",
    "                    min_vec = j\n",
    "    elif (d == 2): # Correlation\n",
    "        for j in range(tempdata.shape[0]):\n",
    "            if (templabel[i] != templabel[j]):\n",
    "                new_dist = distance.correlation(tempdata[i],tempdata[j])\n",
    "                if (new_dist < min_dist):\n",
    "                    min_dist = new_dist\n",
    "                    min_vec = j\n",
    "    elif (d == 3): # Cosine\n",
    "        for j in range(tempdata.shape[0]):\n",
    "            if (templabel[i] != templabel[j]):\n",
    "                new_dist = distance.cosine(tempdata[i],tempdata[j])\n",
    "                if (new_dist < min_dist):\n",
    "                    min_dist = new_dist\n",
    "                    min_vec = j\n",
    "    elif (d == 4): # Euclidean\n",
    "        for j in range(tempdata.shape[0]):\n",
    "            if (templabel[i] != templabel[j]):\n",
    "#                new_dist = distance.euclidean(tempdata[i],tempdata[j]) # slower\n",
    "                new_dist = distance.minkowski(tempdata[i],tempdata[j],2)\n",
    "                if (new_dist < min_dist):\n",
    "                    min_dist = new_dist\n",
    "                    min_vec = j\n",
    "    elif (d == 5): # Minkowski p=3\n",
    "        for j in range(tempdata.shape[0]):\n",
    "            if (templabel[i] != templabel[j]):\n",
    "                new_dist = distance.minkowski(tempdata[i],tempdata[j],3)\n",
    "                \n",
    "                if (new_dist < min_dist):\n",
    "                    min_dist = new_dist\n",
    "                    min_vec = j    \n",
    "    return min_vec\n",
    "\n",
    "def mcov(i): # https://docs.scipy.org/doc/scipy/reference/spatial.distance.html\n",
    "    min_dist = 999999999999999.0\n",
    "    min_vec = -1\n",
    "    for j in range(tempdata.shape[0]):\n",
    "        if (templabel[i] != templabel[j]):\n",
    "            new_dist = distance.minkowski(tempdata[i],tempdata[j],2)\n",
    "            if (new_dist < min_dist):\n",
    "                min_dist = new_dist\n",
    "                min_vec = j                   \n",
    "    return i, min_vec, templabel[i], templabel[min_vec], min_dist\n",
    "\n",
    "def plot_data_distribution_comparison(X1, y1, n1, X2, y2, n2, dataset):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    for label in np.unique(y2):\n",
    "        plt.scatter(X2[y2 == label, 0], X2[y2 == label, 1], \n",
    "                    color=colors[label], label=f'Class {label}', s=1, alpha=0.7)\n",
    "    \n",
    "    plt.xlabel(\"X\")\n",
    "    plt.ylabel(\"Y\")\n",
    "    plt.xlim(0, 1)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.title(\"Data Distribution (D\"+str(dataset+1).zfill(2)+\"): \"+n2,fontsize=16,fontweight='bold')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.savefig(n1+\"-\"+n2+\"-data-distribution-\"+str(dataset+1).zfill(2)+\".pdf\", format=\"pdf\", dpi=None, facecolor=\"w\", edgecolor=\"w\", orientation=\"portrait\", transparent=True, bbox_inches=\"tight\", pad_inches=0.02, metadata=None)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "\n",
    "def plot_class_distribution_comparison(X1, y1, n1, X2, y2, n2, dataset):\n",
    "    classes = np.union1d(np.unique(y1), np.unique(y2))\n",
    "    x = np.arange(len(classes))\n",
    "    width = 0.35\n",
    "    \n",
    "    # Normalize counts\n",
    "    X1_counts = np.array([np.sum(y1 == c) for c in classes]) / len(y1)\n",
    "    X2_counts  = np.array([np.sum(y2 == c) for c in classes]) / len(y2)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    bars1 = plt.bar(x - width/2, X1_counts, width=width, label=n1)\n",
    "    bars2 = plt.bar(x + width/2, X2_counts, width=width, label=n2)\n",
    "\n",
    "    # Add value labels above bars\n",
    "    for i, bar in enumerate(bars1):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                 f'{X1_counts[i]:.2f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "    for i, bar in enumerate(bars2):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                 f'{X2_counts[i]:.2f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "    plt.xlabel(\"Class\")\n",
    "    plt.ylabel(\"Proportion\")\n",
    "    plt.title(\"Class Distribution (D\"+str(dataset+1).zfill(2)+\"): \"+n1+\" vs. \"+n2+\" \", fontsize=16, fontweight='bold')\n",
    "    plt.xticks(x, classes)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.savefig(n1+\"-\"+n2+\"-class-distribution-\"+str(dataset+1).zfill(2)+\".pdf\", format=\"pdf\", dpi=None, facecolor=\"w\", edgecolor=\"w\", orientation=\"portrait\", transparent=True, bbox_inches=\"tight\", pad_inches=0.02, metadata=None)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "# -------------------------------------------------------------------------------------\n",
    "# 3. PARAMETERS\n",
    "# -------------------------------------------------------------------------------------\n",
    "\n",
    "# Switch\n",
    "pca = 0\n",
    "dis_mod = 0\n",
    "bdis_mod = 0\n",
    "mcov_mod = 1\n",
    "bmcov_mod = 1\n",
    "training = 1\n",
    "performance_save = 1\n",
    "figure_save = 1\n",
    "\n",
    "# Values\n",
    "start_set = 0\n",
    "start_type = 0\n",
    "max_type = 7 #18 = MCOV, 19 = BMCOV alwasy +1\n",
    "max_k = 5\n",
    "kdratio = 0.02\n",
    "max_distance = 1\n",
    "reduction_load = 0\n",
    "output_dataset_save = 1\n",
    "encoder = 1 # label = 1, onehot = 2\n",
    "pca_components = 0.99\n",
    "normalization = 1\n",
    "cross_validation = 1\n",
    "cross_validation_fold = 5\n",
    "parallel = 1\n",
    "model = 'knn' # knn, svm, nn\n",
    "\n",
    "d = 4\n",
    "train_ratio = 0.3\n",
    "train_ratio_100k = 0.1\n",
    "train_ratio_500k = 0.01\n",
    "\n",
    "colors = [\n",
    "    '#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd',\n",
    "    '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf',\n",
    "    '#393b79', '#637939', '#8c6d31', '#843c39', '#7b4173',\n",
    "    '#5254a3', '#9c9ede', '#cedb9c', '#e7ba52', '#e7969c',\n",
    "    '#a55194', '#de9ed6', '#f7b6d2', '#c7c7c7', '#dbdb8d',\n",
    "    '#9edae5', '#ffbb78', '#ff9896', '#c5b0d5', '#c49c94',\n",
    "    '#f2b701', '#f28e2b', '#e15759', '#76b7b2', '#59a14f',\n",
    "    '#edc948', '#af7aa1', '#ff9da7', '#9c755f', '#bab0ac',\n",
    "    '#4e79a7', '#f28e2b', '#e15759', '#76b7b2', '#59a14f',\n",
    "    '#edc948', '#b07aa1', '#ff9da7', '#9c755f', '#bab0ac',\n",
    "    '#6b6ecf', '#b5cf6b'\n",
    "]\n",
    "\n",
    "# Labels\n",
    "label_title = ('Accuracy','Precision','Recall','F1-Score','AUC (One-vs-Rest)','AUC (One-vs-One)','Kappa')\n",
    "# Subscript = _, Superscript = ^, use {} for multiple letters\n",
    "label_dataset = ('Synthetic','Banknote','Car','Crowndsourced Mapping','Letter Recognition','Optical Digits Recognition','Pen Digits Recognition','Statlog Landsat','Tic-Tac-Toe','Tezpur Android Malware','Wave Form');\n",
    "# label_traintype = ('ORG','CNN','ENN','RENN','All KNN','TL','OSS','NCL','NM$_1$','NM$_2$','NM$_3$','IHT','CC','RUS','DIS$_{1}$','BDIS$_{1}$','IBP$_{1}$','BIBP$_{1}$','MCOV','BMCOV')\n",
    "label_traintype = ('ORG','PFMCOV','BFMCOV','PAMCOV','BAMCOV','PMCOV','BMCOV')\n",
    "\n",
    "# -------------------------------------------------------------------------------------\n",
    "# 4. LOADING DATA SETS\n",
    "# -------------------------------------------------------------------------------------\n",
    "\n",
    "# Reset performance files(Cross Validation)\n",
    "if (performance_save == 1):\n",
    "\n",
    "    file_removal = glob.glob('*.csv')\n",
    "    for f in file_removal:\n",
    "        os.remove(f)    \n",
    "    file_removal = glob.glob('*.txt')\n",
    "    for f in file_removal:\n",
    "        os.remove(f)    \n",
    "    file_removal = glob.glob('*.pdf')\n",
    "    for f in file_removal:\n",
    "        os.remove(f)\n",
    "\n",
    "# Define the location of the dataset\n",
    "# https://archive.ics.uci.edu/ml/datasets.php\n",
    "# http://www.timeseriesclassification.com/dataset.php\n",
    "files = []\n",
    "file_header = []\n",
    "file_separator = []\n",
    "file_label = []\n",
    "file_big = []\n",
    "\n",
    "# DATA SETS\n",
    "pca = 0\n",
    "files.append(\"https://piyabute.com/data/research/syntactic-wave.csv\"); file_header.append(None); file_separator.append(\"\\t\"); file_label.append(-1); file_big.append(0); # 6A4C\n",
    "files.append(\"https://piyabute.com/data/research/banknote_authentication.csv\"); file_header.append(None); file_separator.append(\",\"); file_label.append(-1); file_big.append(0); # 6A4C\n",
    "files.append(\"https://piyabute.com/data/research/car.data.csv\"); file_header.append(None); file_separator.append(\",\"); file_label.append(-1); file_big.append(0); # 6A4C\n",
    "files.append(\"https://piyabute.com/data/research/crowdsourced_mapping.custom.csv\"); file_header.append(1); file_separator.append(\",\"); file_label.append(0); file_big.append(0); # 6A4C\n",
    "files.append(\"https://piyabute.com/data/research/letter-recognition.data.csv\"); file_header.append(None); file_separator.append(\",\"); file_label.append(0); file_big.append(0); # 8A5C\n",
    "files.append(\"https://piyabute.com/data/research/optdigits.custom.csv\"); file_header.append(None); file_separator.append(\",\"); file_label.append(-1); file_big.append(0); # 64A10C\n",
    "files.append(\"https://piyabute.com/data/research/pendigits.custom.csv\"); file_header.append(None); file_separator.append(\",\"); file_label.append(-1); file_big.append(0); # 16A2C\n",
    "files.append(\"https://piyabute.com/data/research/statlog_landsat.custom.csv\"); file_header.append(None); file_separator.append(\",\"); file_label.append(-1); file_big.append(0); # 36A6C\n",
    "files.append(\"https://piyabute.com/data/research/tic-tac-toe.data.csv\"); file_header.append(None); file_separator.append(\",\"); file_label.append(-1); file_big.append(0); # 6A4C\n",
    "files.append(\"https://piyabute.com/data/research/TUANDROMD.csv\"); file_header.append(0); file_separator.append(\",\"); file_label.append(-1); file_big.append(0); # 36A6C\n",
    "files.append(\"https://piyabute.com/data/research/waveform-+noise.v2.data.csv\"); file_header.append(None); file_separator.append(\",\"); file_label.append(-1); file_big.append(0); # 6A4C\n",
    "\n",
    "max_set = len(files)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------------\n",
    "# 5. LOOP DATASETS\n",
    "# -------------------------------------------------------------------------------------\n",
    "\n",
    "kd_query_miss = np.zeros((max_set),dtype = np.intc)\n",
    "\n",
    "for dataset in range(start_set,max_set):\n",
    "\n",
    "    # -------------------------------------------------------------------------------------\n",
    "    # Initialize performance variables\n",
    "    # -------------------------------------------------------------------------------------\n",
    "    # perf_confusion_matrix=np.zeros(10,4,10,10)\n",
    "    # perf_confusion_matrix = np.zeros((max_set,max_type,max_k),dtype=np.float64)\n",
    "    reduction = np.zeros((max_type),dtype = np.intc)\n",
    "    perf_fit_time = np.zeros((max_type,max_k),dtype = np.float64)\n",
    "    perf_score_time = np.zeros((max_type,max_k),dtype = np.float64)\n",
    "    perf_train_accuracy = np.zeros((max_type,max_k),dtype = np.float64)\n",
    "    perf_train_precision = np.zeros((max_type,max_k),dtype = np.float64)\n",
    "    perf_train_recall = np.zeros((max_type,max_k),dtype = np.float64)\n",
    "    perf_train_f1 = np.zeros((max_type,max_k),dtype = np.float64)\n",
    "    perf_train_aucovr = np.zeros((max_type,max_k),dtype = np.float64)\n",
    "    perf_train_aucovo = np.zeros((max_type,max_k),dtype = np.float64)\n",
    "    perf_train_kappa = np.zeros((max_type,max_k),dtype = np.float64)\n",
    "    time_training = np.zeros((max_type,max_k),dtype = np.float64)\n",
    "    time_reduction = np.zeros((max_type),dtype = np.float64)\n",
    "    mem_training_current = np.zeros((max_type,max_k),dtype = np.float64)\n",
    "    mem_training_peak = np.zeros((max_type,max_k),dtype = np.float64)\n",
    "    k_array = np.zeros((max_k),dtype = np.intc)\n",
    "    d_array = np.zeros((max_set),dtype = np.intc)\n",
    "    l_array = np.zeros((max_type),dtype = np.intc)\n",
    "    l_reduction = np.zeros((max_type),dtype = np.float64)\n",
    "    \n",
    "    # -------------------------------------------------------------------------------------\n",
    "    # Load data file to DataFrame\n",
    "    # -------------------------------------------------------------------------------------\n",
    "\n",
    "    inputfile = files[dataset]\n",
    "    print(\"Dataset:\",dataset)\n",
    "    print(\"Data Source:\",inputfile)\n",
    "    if (inputfile[-2:] == \"gz\"):\n",
    "        df = pd.read_csv(inputfile, header = file_header[dataset], sep = file_separator[dataset], compression=\"gzip\")\n",
    "    elif (inputfile[-3:] == \".zip\"):\n",
    "        df = pd.read_csv(inputfile, header = file_header[dataset], sep = file_separator[dataset], compression=\"zip\")\n",
    "    elif (inputfile[-4:] == \".xls\") or (inputfile[-4:] == \"xlsx\"):\n",
    "        df = pd.read_excel(inputfile, header = file_header[dataset])      \n",
    "    else:\n",
    "        df = pd.read_csv(inputfile, header = file_header[dataset], sep = file_separator[dataset])\n",
    "\n",
    "    # -------------------------------------------------------------------------------------\n",
    "    # Pre-process DataFrame\n",
    "    # -------------------------------------------------------------------------------------\n",
    "        \n",
    "    # Drop specific rows\n",
    "    if (inputfile == \"https://piyabute.com/data/research/TUANDROMD.txt\"): df.drop(df.index[2533], inplace=True)\n",
    "    if (inputfile == \"https://piyabute.com/data/research/yeast.data\"): df.drop(labels=None, axis=0, inplace=True)\n",
    "   \n",
    "    # Stratified sampling\n",
    "    if (file_big[dataset] > 0): \n",
    "        # Generate column index\n",
    "        df.columns = df.columns.map(str)\n",
    "        df = df.groupby(df.columns[file_label[dataset]], group_keys=False).apply(lambda x: x.sample(file_big[dataset]))\n",
    "    \n",
    "    # Categorical to Numerical\n",
    "    \n",
    "    if (encoder == 1):\n",
    "        # Initialize the LabelEncoder\n",
    "        labelencoder = LabelEncoder()\n",
    "        # Loop through each column in the DataFrame and apply label encoding\n",
    "        for column in df.columns:\n",
    "            df[column] = labelencoder.fit_transform(df[column])\n",
    "    elif (encoder == 2): # Incomplete!\n",
    "        # Convert the label column into one hot encoding\n",
    "        one_hot = pd.get_dummies(df.iloc[:, file_label[dataset]])\n",
    "        # Drop the label column\n",
    "        last_column_name = df.columns[file_label[dataset]]\n",
    "        df.drop(columns=[last_column_name], inplace=True)\n",
    "        df = df.join(one_hot)\n",
    "        \n",
    "    # Fill missing values (slow for big data) with the mode (most frequent value) of the column\n",
    "    for column in df.columns:\n",
    "        df.fillna({column: df[column].mode()[0]}, inplace=True)\n",
    "\n",
    "    # Convert dataframe to numpy\n",
    "    raw = df.to_numpy()\n",
    "\n",
    "    # Extract the label attribute\n",
    "    trainsetlabel = raw[:,file_label[dataset]]\n",
    "    trainsetlabel = trainsetlabel.astype(int)\n",
    "\n",
    "    # Remove the label attribute\n",
    "    trainsetdata = np.delete(raw,file_label[dataset],1)\n",
    "    \n",
    "    # Calculate PCA\n",
    "    if (pca == 1):\n",
    "        postpca = PCA(n_components=pca_components)\n",
    "        traindata = postpca.fit_transform(trainsetdata)\n",
    "        trainlabel = trainsetlabel\n",
    "    else:\n",
    "        traindata = trainsetdata\n",
    "        trainlabel = trainsetlabel\n",
    "    \n",
    "    # Normalize normal attributes using MinMaxScaler\n",
    "    if (normalization == 1):\n",
    "        scaler = MinMaxScaler(feature_range=(0.01, 0.99))\n",
    "        scaler.fit(traindata)\n",
    "        traindata = scaler.transform(traindata)\n",
    "        trainlabel = trainlabel - np.amin(trainlabel)\n",
    " \n",
    "    # -------------------------------------------------------------------------------------\n",
    "    # Prepare initial training set and test set\n",
    "    # -------------------------------------------------------------------------------------\n",
    "\n",
    "    # Construct the training set and test set\n",
    "    if (cross_validation == 1):\n",
    "        testdata = []\n",
    "        testlabel = []\n",
    "        trainsize = traindata.shape[0]\n",
    "        traindimension = traindata.shape[1]\n",
    "        reduction[0] = trainsize\n",
    "    else:\n",
    "        if ((traindata.shape[0]*traindata.shape[1])<100000):\n",
    "            traindata, testdata, trainlabel, testlabel = train_test_split(traindata, trainlabel, train_size=train_ratio, stratify=trainlabel, random_state=42)\n",
    "        elif ((traindata.shape[0]*traindata.shape[1])<500000):\n",
    "            traindata, testdata, trainlabel, testlabel = train_test_split(traindata, trainlabel, train_size=train_ratio_100k, stratify=trainlabel, random_state=42)\n",
    "        else:\n",
    "            traindata, testdata, trainlabel, testlabel = train_test_split(traindata, trainlabel, train_size=train_ratio_500k, stratify=trainlabel, random_state=42)\n",
    "        trainsize = traindata.shape[0]\n",
    "        traindimension = traindata.shape[1]\n",
    "        testsize = testdata.shape[0]\n",
    "        testdimension = testdata.shape[1]\n",
    "        reduction[0] = trainsize     \n",
    "\n",
    "    print(\"Preprocessing: Attribute =\",traindimension,\"(PCA)\" if pca==1 else \"(Non-PCA)\",\"(Norm)\" if normalization==1 else \"(Non-Norm)\")\n",
    "    print(\"Preprocessing: Sample    =\",trainsize)\n",
    "    print(\"Preprocessing: Class     =\", len(np.unique(trainlabel)), np.unique(trainlabel))\n",
    "    reduction_class = np.zeros((max_type,len(np.unique(trainlabel))),dtype = np.float64)\n",
    "\n",
    "    # Print class balance\n",
    "    if (cross_validation == 1):\n",
    "        unique_train, counts_train = np.unique(trainlabel, return_counts=True)\n",
    "        print(\"Preprocessing: Train     =\",dict(zip(unique_train, counts_train)))\n",
    "    else:\n",
    "        unique_test, counts_test = np.unique(testlabel, return_counts=True)\n",
    "        print(\"Preprocessing: Test      =\",dict(zip(unique_test, counts_test)))\n",
    "\n",
    "    # -------------------------------------------------------------------------------------\n",
    "    # Generate DISTANCE-BASED INSTANCE SELECTION (DIS) training sets \n",
    "    # -------------------------------------------------------------------------------------\n",
    "\n",
    "    if (dis_mod == 1):\n",
    "        tempset = raw\n",
    "        tempdata = traindata\n",
    "        templabel = trainlabel\n",
    "        scatter_x = []\n",
    "        scatter_y = []\n",
    "    \n",
    "        # Start clock\n",
    "        time_start = time.perf_counter()\n",
    "    \n",
    "        # Parallel or serialized reduction\n",
    "        if (parallel == 1):\n",
    "            if __name__ == \"__main__\":\n",
    "                d = 4; # 4 = Euclidean\n",
    "                ProcessList = Parallel(n_jobs=num_cores)(delayed(find_minimum_distance)(d,i) for i in range(tempdata.shape[0]))\n",
    "        else:\n",
    "            ProcessList = [0]*tempdata.shape[0]\n",
    "            for i in range(0,tempdata.shape[0]):\n",
    "                ProcessList[i] = find_minimum_distance(i)\n",
    "                print(i,templabel[i],ProcessList[i],templabel[ProcessList[i]])\n",
    "    \n",
    "        # Remove duplicates\n",
    "        dis_vec = np.unique(ProcessList)\n",
    "    \n",
    "        # Stop clock\n",
    "        time_stop = time.perf_counter()\n",
    "    \n",
    "        # Save execution time\n",
    "        time_reduction[14] = time_stop - time_start\n",
    "    \n",
    "        # Construct reduced data set\n",
    "        disdata = tempdata[dis_vec]\n",
    "        dislabel = templabel[dis_vec]\n",
    "        \n",
    "        # Level 1 only\n",
    "        disdata1 = disdata\n",
    "        dislabel1 = dislabel\n",
    "        tempsetbefore = tempdata.shape[0]\n",
    "        tempdata1 = np.delete(tempdata,dis_vec,0)\n",
    "        templabel1 = np.delete(templabel,dis_vec,0)\n",
    "        tempdata = tempdata1\n",
    "        templabel = templabel1\n",
    "        tempsetafter = tempdata1.shape[0]\n",
    "    \n",
    "        print(\"DIS Reduction: Time = \",\"{:0.2f}\".format(time_stop - time_start),\" seconds / Size = \",disdata.shape[0],\" / Original = \",tempsetbefore,\" / Remain = \", tempsetafter, sep = \"\")\n",
    "    \n",
    "        # Save output data sets\n",
    "        if (output_dataset_save == 1):\n",
    "            if (pca == 1):\n",
    "                outputfile = Path(inputfile).stem+\"_pca_dis.txt\"\n",
    "            else:\n",
    "                outputfile = Path(inputfile).stem+\"_dis.txt\"\n",
    "            with open(outputfile, \"ab\") as file:\n",
    "                np.savetxt(file, np.c_[disdata1, np.array(dislabel1)], delimiter=\",\", fmt=\"%1.4f\")            \n",
    "                print(\"Saving reduced dataset (DIS) to \"+outputfile, sep=\" \")\n",
    "\n",
    "    # -------------------------------------------------------------------------------------\n",
    "    # Generate BOOSTING DISTANCED-BASED INSTANCE SELECTION (BDIS) training sets \n",
    "    # -------------------------------------------------------------------------------------\n",
    "\n",
    "    if (bdis_mod == 1):\n",
    "        tempset = raw\n",
    "        tempdata = traindata\n",
    "        templabel = trainlabel\n",
    "        scatter_x = []\n",
    "        scatter_y = []\n",
    "    \n",
    "        # Initialize arrays to store the nearest instance and its distance for each instance\n",
    "        bdis_vec = np.zeros(len(tempdata), dtype=np.int32)\n",
    "        bdis_dis = np.ones(len(tempdata)) * np.inf\n",
    "    \n",
    "        # Start clock\n",
    "        time_start = time.perf_counter()\n",
    "    \n",
    "        # Create a cKDTree object from the feature data\n",
    "        tree = cKDTree(tempdata)\n",
    "\n",
    "        # Define the k-d query size\n",
    "        kdsize = max(len(np.unique(templabel)),int(len(templabel)*kdratio))\n",
    "        \n",
    "        # Loop through each instance in the feature data\n",
    "        for i in range(len(tempdata)):\n",
    "            # Find the distance and index of the nearest neighbor\n",
    "            min_dis, min_vec = tree.query(tempdata[i], k=kdsize)\n",
    "    \n",
    "            # Check if the nearest neighbor belongs to a different class\n",
    "            for j in range(1, len(min_vec)):\n",
    "                if (templabel[i] != templabel[min_vec[j]]):\n",
    "                    bdis_vec[i] = min_vec[j]\n",
    "                    bdis_dis[i] = min_dis[j]\n",
    "                    break\n",
    "    \n",
    "        # Remove duplicates\n",
    "        bdis_vec = np.unique(bdis_vec)\n",
    "    \n",
    "        # Stop clock\n",
    "        time_stop = time.perf_counter()\n",
    "    \n",
    "        # Save execution time\n",
    "        time_reduction[15] = time_stop - time_start\n",
    "    \n",
    "        # Construct reduced data set\n",
    "        bdisdata = tempdata[bdis_vec]\n",
    "        bdislabel = templabel[bdis_vec]\n",
    "    \n",
    "        # Level 1 only\n",
    "        bdisdata1 = bdisdata\n",
    "        bdislabel1 = bdislabel\n",
    "        tempsetbefore = tempdata.shape[0]\n",
    "        tempdata1 = np.delete(tempdata,bdis_vec,0)\n",
    "        templabel1 = np.delete(templabel,bdis_vec,0)\n",
    "        tempdata = tempdata1\n",
    "        templabel = templabel1\n",
    "        tempsetafter = tempdata1.shape[0]\n",
    "    \n",
    "        print(\"BDIS Reduction (k=\",kdsize,\"): Time = \",\"{:0.2f}\".format(time_stop - time_start),\" seconds / Size = \",bdisdata.shape[0],\" / Original = \",tempsetbefore,\" / Remain = \", tempsetafter, sep = \"\")\n",
    "    \n",
    "        # Save output data sets\n",
    "        if (output_dataset_save == 1):\n",
    "            if (pca == 1):\n",
    "                outputfile = Path(inputfile).stem+\"_pca_bdis.txt\"\n",
    "            else:\n",
    "                outputfile = Path(inputfile).stem+\"_bdis.txt\"\n",
    "            with open(outputfile, \"ab\") as file:\n",
    "                np.savetxt(file, np.c_[bdisdata1, np.array(bdislabel1)], delimiter=\",\", fmt=\"%1.4f\")            \n",
    "                print(\"Saving reduced dataset (BDIS) to \"+outputfile, sep=\" \")\n",
    "\n",
    "    # -------------------------------------------------------------------------------------\n",
    "    # Generate MULTI-CLASS CONTOUR PRESERVING CLASSIFICATION (MCOV) training sets\n",
    "    # -------------------------------------------------------------------------------------\n",
    "\n",
    "    if (mcov_mod == 1):\n",
    "        tempset = raw\n",
    "        tempdata = traindata\n",
    "        templabel = trainlabel\n",
    "        scatter_x = []\n",
    "        scatter_y = []\n",
    "\n",
    "        # Start clock FMCOV\n",
    "        time_start = time.perf_counter()\n",
    "    \n",
    "        # Parallel or serialized reduction\n",
    "        if (parallel == 1):\n",
    "            if __name__ == \"__main__\":\n",
    "                ProcessList = Parallel(n_jobs=num_cores)(delayed(mcov)(i) for i in range(tempdata.shape[0]))\n",
    "\n",
    "        # Synthesize FMCOV\n",
    "        fmcovkappa = 0.40\n",
    "        fmcovdata = np.array([(1 - fmcovkappa) * tempdata[x[0]] + fmcovkappa * tempdata[x[1]] for x in ProcessList])       \n",
    "        fmcovlabel = np.array([x[2] for x in ProcessList])\n",
    "\n",
    "        # Remove duplicates\n",
    "        fmcov = np.column_stack((fmcovdata,fmcovlabel))\n",
    "        fmcov = np.unique(fmcov, axis=0)\n",
    "\n",
    "        # Split fmcov back into fmcovdata and fmcovlabel\n",
    "        fmcovdata = fmcov[:, :-1]\n",
    "        fmcovlabel = fmcov[:, -1].astype(int)\n",
    "\n",
    "        # Stop clock FMCOV\n",
    "        time_stop = time.perf_counter()\n",
    "        \n",
    "        # Save execution time FMCOV\n",
    "        time_reduction[1] = time_stop - time_start\n",
    "\n",
    "        # Start clock AMCOV\n",
    "        time_start = time.perf_counter()\n",
    "\n",
    "        # Synthesize AMCOV\n",
    "        amcovkappa = 0.40\n",
    "        amcovdist = {}\n",
    "\n",
    "        for x in ProcessList:\n",
    "            key = (x[1])  # Create a tuple key\n",
    "            if key not in amcovdist or x[4] < amcovdist[key]:\n",
    "                amcovdist[key] = x[4]\n",
    "\n",
    "        amcovdata, amcovlabel = zip(*[\n",
    "            (\n",
    "                amcovkappa * amcovdist[x[1]] / x[4] * tempdata[x[0]] +\n",
    "                (1 - (amcovkappa * amcovdist[x[1]] / x[4])) * tempdata[x[1]],\n",
    "                x[3]\n",
    "            )\n",
    "            for x in ProcessList if x[4] != 0\n",
    "        ])\n",
    "        \n",
    "        # Convert them back to numpy arrays\n",
    "        amcovdata = np.array(amcovdata)\n",
    "        amcovlabel = np.array(amcovlabel)\n",
    "\n",
    "        amcov = np.column_stack((amcovdata, amcovlabel))\n",
    "        amcov = np.unique(amcov, axis=0)\n",
    "        \n",
    "        # Split amcov back into amcovdata and amcovlabel\n",
    "        amcovdata = amcov[:, :-1]\n",
    "        amcovlabel = amcov[:, -1].astype(int)\n",
    "       \n",
    "        # Stop clock AMCOV\n",
    "        time_stop = time.perf_counter()\n",
    "        \n",
    "        # Save execution time AMCOV\n",
    "        time_reduction[2] = time_stop - time_start\n",
    "\n",
    "        # Save execution time MCOV\n",
    "        time_reduction[3] = time_reduction[1] + time_reduction[2]\n",
    "   \n",
    "        mcovdata = np.concatenate((fmcovdata,amcovdata), axis=0)\n",
    "        mcovlabel = np.concatenate((fmcovlabel,amcovlabel), axis=0)\n",
    "        tempsetbefore = tempdata.shape[0]\n",
    "        tempsetafter = mcovdata.shape[0]\n",
    "    \n",
    "        print(\"MCOV Synthesization: Time = \",\"{:0.2f}\".format(time_stop - time_start),\" seconds / Size = \",mcovdata.shape[0],\" / Original = \",tempsetbefore,\" / Remain = \", tempsetafter, sep = \"\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------------------\n",
    "    # Generate BOOSTING MULTI-CLASS CONTOUR PRESERVING CLASSIFICATION (BMCOV) training sets\n",
    "    # -------------------------------------------------------------------------------------\n",
    "\n",
    "    if (bmcov_mod == 1):\n",
    "\n",
    "        tempset = raw\n",
    "        tempdata = traindata\n",
    "        templabel = trainlabel\n",
    "        scatter_x = []\n",
    "        scatter_y = []\n",
    "    \n",
    "        # Start clock BFMCOV\n",
    "        time_start = time.perf_counter()\n",
    "    \n",
    "        # Create a cKDTree object from the feature data\n",
    "        tree = cKDTree(tempdata)\n",
    "\n",
    "        # Define the k-d query size\n",
    "        kdsize = max(len(np.unique(templabel)),int(len(templabel)*kdratio))\n",
    "        \n",
    "        # Loop through each instance in the feature data\n",
    "        ProcessList = []\n",
    "        \n",
    "        for i in range(len(tempdata)):\n",
    "            # Find the distance and index of the nearest neighbor\n",
    "            min_dis, min_vec = tree.query(tempdata[i], k=kdsize)\n",
    "    \n",
    "            # Check if the nearest neighbor belongs to a different class\n",
    "            hit = False\n",
    "            for j in range(1, len(min_vec)):\n",
    "                if (templabel[i] != templabel[min_vec[j]]):\n",
    "                    ProcessList.append([np.int64(i),np.int64(min_vec[j]),templabel[i],templabel[min_vec[j]],min_dis[j]])\n",
    "                    hit = True\n",
    "                    break\n",
    "            if hit == False:\n",
    "                kd_query_miss[dataset] += 1\n",
    "    \n",
    "        # Synthesize fmcov\n",
    "        bfmcovkappa = 0.40\n",
    "        bfmcovdata = np.array([(1 - bfmcovkappa) * tempdata[x[0]] + bfmcovkappa * tempdata[x[1]] for x in ProcessList])\n",
    "        bfmcovlabel = np.array([x[2] for x in ProcessList])\n",
    "\n",
    "        # Remove duplicates\n",
    "        bfmcov = np.column_stack((bfmcovdata,bfmcovlabel))\n",
    "        bfmcov = np.unique(bfmcov, axis=0)\n",
    "\n",
    "        # Split fmcov back into fmcovdata and fmcovlabel\n",
    "        bfmcovdata = bfmcov[:, :-1]\n",
    "        bfmcovlabel = bfmcov[:, -1].astype(int)\n",
    "        \n",
    "        # Stop clock BFMCOV\n",
    "        time_stop = time.perf_counter()\n",
    "        \n",
    "        # Save execution time BFMCOV\n",
    "        time_reduction[4] = time_stop - time_start\n",
    "\n",
    "        # Start clock BAMCOV\n",
    "        time_start = time.perf_counter()\n",
    "\n",
    "        # Synthesize BAMCOV\n",
    "        bamcovkappa = 0.40\n",
    "        bamcovdist = {}\n",
    "\n",
    "        for x in ProcessList:\n",
    "            key = (x[1])  # Create a tuple key\n",
    "            if key not in bamcovdist or x[4] < bamcovdist[key]:\n",
    "                bamcovdist[key] = x[4]\n",
    "\n",
    "        # Create a dictionary to store phi(i) mappings\n",
    "        phi_dict = {x[0]: x[1] for x in ProcessList}\n",
    "        \n",
    "        bamcovdata, bamcovlabel = zip(*[\n",
    "            (\n",
    "                bamcovkappa * bamcovdist[x[1]] / x[4] * tempdata[x[0]] +\n",
    "                (1 - (bamcovkappa * bamcovdist[x[1]] / x[4])) * tempdata[x[1]],\n",
    "                x[3]\n",
    "            )\n",
    "            for x in ProcessList if x[4] != 0 and phi_dict.get(x[1], -1) != x[0]  # phi(phi(i)) != i\n",
    "        ])\n",
    "        \n",
    "        # Convert them back to numpy arrays\n",
    "        bamcovdata = np.array(amcovdata)\n",
    "        bamcovlabel = np.array(amcovlabel)\n",
    "\n",
    "        bamcov = np.column_stack((bamcovdata, bamcovlabel))\n",
    "        bamcov = np.unique(bamcov, axis=0)\n",
    "        \n",
    "        # Split amcov back into amcovdata and amcovlabel\n",
    "        bamcovdata = bamcov[:, :-1]\n",
    "        bamcovlabel = bamcov[:, -1].astype(int)\n",
    "       \n",
    "        # Stop clock BAMCOV\n",
    "        time_stop = time.perf_counter()\n",
    "        \n",
    "        # Save execution time BAMCOV\n",
    "        time_reduction[5] = time_stop - time_start\n",
    "\n",
    "        # Save execution time BMCOV\n",
    "        time_reduction[6] = time_reduction[4] + time_reduction[5]\n",
    "   \n",
    "        bmcovdata = np.concatenate((bfmcovdata,bamcovdata), axis=0)\n",
    "        bmcovlabel = np.concatenate((bfmcovlabel,bamcovlabel), axis=0)\n",
    "        tempsetbefore = tempdata.shape[0]\n",
    "        tempsetafter = bmcovdata.shape[0]\n",
    "    \n",
    "        print(\"BMCOV Synthesization: kd = \",kdsize,\" / Time = \",\"{:0.2f}\".format(time_stop - time_start),\" seconds / Size = \",bmcovdata.shape[0],\" / Original = \",tempsetbefore,\" / Remain = \", tempsetafter, sep = \"\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------------------\n",
    "    # Generate plots and save output and meta\n",
    "    # -------------------------------------------------------------------------------------\n",
    "        \n",
    "    if (mcov_mod == 1):\n",
    "        # Plot the MCOV variants\n",
    "        plot_data_distribution_comparison(traindata, trainlabel, \"ORG\", mcovdata, mcovlabel, \"PMCOV\", dataset)\n",
    "        plot_data_distribution_comparison(traindata, trainlabel, \"ORG\", fmcovdata, fmcovlabel, \"PFMCOV\", dataset)\n",
    "        plot_data_distribution_comparison(traindata, trainlabel, \"ORG\", amcovdata, amcovlabel, \"PAMCOV\", dataset)\n",
    "        \n",
    "        # Plot the class distribution comparison\n",
    "        plot_class_distribution_comparison(traindata, trainlabel, \"ORG\", mcovdata, mcovlabel, \"PMCOV\", dataset)\n",
    "        plot_class_distribution_comparison(traindata, trainlabel, \"ORG\", fmcovdata, fmcovlabel, \"PFMCOV\", dataset)\n",
    "        plot_class_distribution_comparison(traindata, trainlabel, \"ORG\", amcovdata, amcovlabel, \"PAMCOV\", dataset)\n",
    "\n",
    "        unique_mcov, counts_mcov = np.unique(mcovlabel, return_counts=True)\n",
    "        unique_fmcov, counts_fmcov = np.unique(fmcovlabel, return_counts=True)\n",
    "        unique_amcov, counts_amcov = np.unique(amcovlabel, return_counts=True)\n",
    "\n",
    "        outputfile = \"data-fmcov-\"+str(dataset+1).zfill(2)+\".txt\"\n",
    "        with open(outputfile, \"ab\") as file:\n",
    "            np.savetxt(file, np.c_[fmcovdata, np.array(fmcovlabel)], delimiter=\",\", fmt=\"%1.4f\")            \n",
    "            print(\"Saving reduced dataset (FMCOV) to \"+outputfile, sep=\" \")    \n",
    "\n",
    "        outputfile = \"data-amcov-\"+str(dataset+1).zfill(2)+\".txt\"\n",
    "        with open(outputfile, \"ab\") as file:\n",
    "            np.savetxt(file, np.c_[amcovdata, np.array(amcovlabel)], delimiter=\",\", fmt=\"%1.4f\")            \n",
    "            print(\"Saving reduced dataset (AMCOV) to \"+outputfile, sep=\" \")    \n",
    "\n",
    "        outputfile = \"data-mcov-\"+str(dataset+1).zfill(2)+\".txt\"\n",
    "        with open(outputfile, \"ab\") as file:\n",
    "            np.savetxt(file, np.c_[mcovdata, np.array(mcovlabel)], delimiter=\",\", fmt=\"%1.4f\")            \n",
    "            print(\"Saving reduced dataset (MCOV) to \"+outputfile, sep=\" \")    \n",
    "\n",
    "        outputfile = \"class-balance-fmcov.txt\"\n",
    "        with open(outputfile, \"ab\") as file:\n",
    "            np.savetxt(file, counts_fmcov.reshape(1,counts_fmcov.shape[0]), delimiter=\"\\t\", fmt=\"%0.0f\")\n",
    "            print(\"Saving class balance (FMCOV) to \"+outputfile, sep=\" \")    \n",
    "\n",
    "        outputfile = \"class-balance-amcov.txt\"\n",
    "        with open(outputfile, \"ab\") as file:\n",
    "            np.savetxt(file, counts_amcov.reshape(1,counts_amcov.shape[0]), delimiter=\"\\t\", fmt=\"%0.0f\")\n",
    "            print(\"Saving class balance (AMCOV) to \"+outputfile, sep=\" \")    \n",
    "\n",
    "        outputfile = \"class-balance-mcov.txt\"\n",
    "        with open(outputfile, \"ab\") as file:\n",
    "            np.savetxt(file, counts_mcov.reshape(1,counts_mcov.shape[0]), delimiter=\"\\t\", fmt=\"%0.0f\")\n",
    "            print(\"Saving class balance (MCOV) to \"+outputfile, sep=\" \")    \n",
    "    \n",
    "    if (bmcov_mod == 1):      \n",
    "        # Plot the BMCOV variants\n",
    "        plot_data_distribution_comparison(traindata, trainlabel, \"ORG\", bmcovdata, bmcovlabel, \"BMCOV\", dataset)\n",
    "        plot_data_distribution_comparison(traindata, trainlabel, \"ORG\", bfmcovdata, bfmcovlabel, \"BFMCOV\", dataset)\n",
    "        plot_data_distribution_comparison(traindata, trainlabel, \"ORG\", bamcovdata, bamcovlabel, \"BAMCOV\", dataset)\n",
    "        \n",
    "        # Plot the class distribution comparison\n",
    "        plot_class_distribution_comparison(traindata, trainlabel, \"ORG\", bmcovdata, bmcovlabel, \"BMCOV\", dataset)\n",
    "        plot_class_distribution_comparison(traindata, trainlabel, \"ORG\", bfmcovdata, bfmcovlabel, \"BFMCOV\", dataset)\n",
    "        plot_class_distribution_comparison(traindata, trainlabel, \"ORG\", bamcovdata, bamcovlabel, \"BAMCOV\", dataset)\n",
    "        \n",
    "        # Calculate output meta\n",
    "        unique_bmcov, counts_bmcov = np.unique(bmcovlabel, return_counts=True)\n",
    "        unique_bfmcov, counts_bfmcov = np.unique(bfmcovlabel, return_counts=True)\n",
    "        unique_bamcov, counts_bamcov = np.unique(bamcovlabel, return_counts=True)\n",
    "\n",
    "        # outputfile = Path(inputfile).stem+\"-bfmcov.txt\"\n",
    "        outputfile = \"data-bfmcov-\"+str(dataset+1).zfill(2)+\".txt\"\n",
    "        with open(outputfile, \"ab\") as file:\n",
    "            np.savetxt(file, np.c_[bfmcovdata, np.array(bfmcovlabel)], delimiter=\",\", fmt=\"%1.4f\")            \n",
    "            print(\"Saving reduced dataset (BFMCOV) to \"+outputfile, sep=\" \")    \n",
    "\n",
    "        outputfile = \"data-bamcov-\"+str(dataset+1).zfill(2)+\".txt\"\n",
    "        with open(outputfile, \"ab\") as file:\n",
    "            np.savetxt(file, np.c_[bamcovdata, np.array(bamcovlabel)], delimiter=\",\", fmt=\"%1.4f\")            \n",
    "            print(\"Saving reduced dataset (BAMCOV) to \"+outputfile, sep=\" \")    \n",
    "\n",
    "        outputfile = \"data-bmcov-\"+str(dataset+1).zfill(2)+\".txt\"\n",
    "        with open(outputfile, \"ab\") as file:\n",
    "            np.savetxt(file, np.c_[bmcovdata, np.array(bmcovlabel)], delimiter=\",\", fmt=\"%1.4f\")            \n",
    "            print(\"Saving reduced dataset (BMCOV) to \"+outputfile, sep=\" \")    \n",
    "\n",
    "        outputfile = \"class-balance-bfmcov.txt\"\n",
    "        with open(outputfile, \"ab\") as file:\n",
    "            np.savetxt(file, counts_bfmcov.reshape(1,counts_bfmcov.shape[0]), delimiter=\"\\t\", fmt=\"%0.0f\")\n",
    "            print(\"Saving class balance (BFMCOV) to \"+outputfile, sep=\" \")    \n",
    "\n",
    "        outputfile = \"class-balance-bamcov.txt\"\n",
    "        with open(outputfile, \"ab\") as file:\n",
    "            np.savetxt(file, counts_bamcov.reshape(1,counts_bamcov.shape[0]), delimiter=\"\\t\", fmt=\"%0.0f\")\n",
    "            print(\"Saving class balance (BAMCOV) to \"+outputfile, sep=\" \")    \n",
    "\n",
    "        outputfile = \"class-balance-bmcov.txt\"\n",
    "        with open(outputfile, \"ab\") as file:\n",
    "            np.savetxt(file, counts_bmcov.reshape(1,counts_bmcov.shape[0]), delimiter=\"\\t\", fmt=\"%0.0f\")\n",
    "            print(\"Saving class balance (BMCOV) to \"+outputfile, sep=\" \")    \n",
    " \n",
    "    # -------------------------------------------------------------------------------------\n",
    "    # Generate final training sets\n",
    "    # -------------------------------------------------------------------------------------\n",
    "\n",
    "    # https://imbalanced-learn.org/\n",
    "    for traintype in range(0,max_type):\n",
    "        if traintype == 0: # Original\n",
    "            xready = traindata\n",
    "            tready = trainlabel\n",
    "            temp, reduction_class_org = np.unique(tready, return_counts=True)\n",
    "        if traintype == 1: # FMCOV\n",
    "            xready = fmcovdata\n",
    "            tready = fmcovlabel\n",
    "        if traintype == 2: # BFMCOV\n",
    "            xready = bfmcovdata\n",
    "            tready = bfmcovlabel\n",
    "        if traintype == 3: # AMCOV\n",
    "            xready = amcovdata\n",
    "            tready = amcovlabel\n",
    "        if traintype == 4: # BAMCOV\n",
    "            xready = bamcovdata\n",
    "            tready = bamcovlabel\n",
    "        if traintype == 5: # MCOV\n",
    "            xready = mcovdata\n",
    "            tready = mcovlabel\n",
    "        if traintype == 6: # BMCOV\n",
    "            xready = bmcovdata\n",
    "            tready = bmcovlabel\n",
    "\n",
    "        unique_tready, counts_tready = np.unique(tready, return_counts=True)\n",
    "\n",
    "        # -------------------------------------------------------------------------------------\n",
    "        # Generate BDIS and BIBP using OverSampling\n",
    "        # -------------------------------------------------------------------------------------\n",
    "\n",
    "        # OverSampling for DIS and IBP\n",
    "        if (traintype == 16 or traintype ==17):\n",
    "\n",
    "            # Maintain minimum values for n-fold cross-validation\n",
    "            time_start = time.perf_counter()\n",
    "\n",
    "            unique_tready, counts_tready = np.unique(tready, return_counts=True)\n",
    "            mydict = dict(zip(unique_tready,counts_tready))\n",
    "            print(\"Oversampling (Pre) : Set=\",dataset,\"; Type=\",traintype,\" \",label_traintype[traintype],\"; sample=\", len(tready),\" ({:0.2f}\".format(len(tready)/len(trainlabel)*100), \"%) \", dict(zip(unique_tready, counts_tready)), sep = \"\");\n",
    "            for i in mydict: \n",
    "                mydict[i]=np.maximum(mydict[i],cross_validation_fold*max_k*4)\n",
    "\n",
    "            # mymodel_smote = SMOTE(n_jobs=-1,sampling_strategy=mydict)\n",
    "            mymodel_ros = RandomOverSampler(sampling_strategy=mydict)\n",
    "            xready, tready = mymodel_ros.fit_resample(xready, tready)\n",
    "            unique_tready, counts_tready = np.unique(tready, return_counts=True)\n",
    "\n",
    "            time_stop = time.perf_counter()\n",
    "            time_reduction[traintype] = time_reduction[traintype] + time_stop - time_start\n",
    "\n",
    "            print(\"Oversampling (Post): Set=\",dataset,\"; Type=\",traintype,\" \",label_traintype[traintype],\"; sample=\", len(tready),\" ({:0.2f}\".format(len(tready)/len(trainlabel)*100), \"%) \", dict(zip(unique_tready, counts_tready)), sep = \"\");\n",
    "        else:\n",
    "            # Maintain minimum values for n-fold cross-validation\n",
    "            time_start = time.perf_counter()\n",
    "\n",
    "            unique_tready, counts_tready = np.unique(tready, return_counts=True)\n",
    "            mydict = dict(zip(unique_tready,counts_tready))\n",
    "            print(\"Oversampling (Pre) : Set=\",dataset,\"; Type=\",traintype,\" \",label_traintype[traintype],\"; sample=\", len(tready),\" ({:0.2f}\".format(len(tready)/len(trainlabel)*100), \"%) \", dict(zip(unique_tready, counts_tready)), sep = \"\");\n",
    "            for i in mydict: \n",
    "                mydict[i]=np.maximum(mydict[i],cross_validation_fold)\n",
    "\n",
    "            # mymodel_smote = SMOTE(n_jobs=-1,sampling_strategy=mydict)\n",
    "            mymodel_ros = RandomOverSampler(sampling_strategy=mydict)\n",
    "            xready, tready = mymodel_ros.fit_resample(xready, tready)\n",
    "            unique_tready, counts_tready = np.unique(tready, return_counts=True)\n",
    "\n",
    "            time_stop = time.perf_counter()\n",
    "            time_reduction[traintype] = time_reduction[traintype] + time_stop - time_start\n",
    "\n",
    "            print(\"Oversampling (Post): Set=\",dataset,\"; Type=\",traintype,\" \",label_traintype[traintype],\"; sample=\", len(tready),\" ({:0.2f}\".format(len(tready)/len(trainlabel)*100), \"%) \", dict(zip(unique_tready, counts_tready)), sep = \"\");\n",
    "                   \n",
    "        if (len(np.unique(tready))<len(np.unique(trainlabel))):\n",
    "            print(\"Training: Cannot maintain all class labels!\")\n",
    "            # continue\n",
    "\n",
    "        reduction[traintype] = len(tready)\n",
    "\n",
    "        # Fixing empty class\n",
    "        j = 0;\n",
    "        for i in unique_tready:\n",
    "            reduction_class[traintype][i] = counts_tready[j]\n",
    "            j += 1\n",
    "    \n",
    "        # -------------------------------------------------------------------------------------\n",
    "        # Plot training sets\n",
    "        # -------------------------------------------------------------------------------------\n",
    "\n",
    "        if (figure_save == 1):\n",
    "            if (traindimension == 2):\n",
    "                scatter_x = xready[:,0]\n",
    "                scatter_y = xready[:,1]\n",
    "                scatter_x_min = 0\n",
    "                scatter_x_max = 1\n",
    "                scatter_y_min = 0\n",
    "                scatter_y_max = 1\n",
    "                group = tready\n",
    "\n",
    "                plt.figure(figsize=(8, 6))\n",
    "                for label in np.unique(tready):\n",
    "                    mask = tready == label\n",
    "                    plt.scatter(scatter_x[mask], scatter_y[mask], \n",
    "                                color=colors[label], label=f'Class {label}', s=1, alpha=0.7)\n",
    "                \n",
    "                plt.xlabel('X')\n",
    "                plt.ylabel('Y')\n",
    "                plt.xlim([scatter_x_min,scatter_x_max])\n",
    "                plt.ylim([scatter_y_min,scatter_y_max])\n",
    "                plt.title(label_traintype[traintype],fontsize=16,fontweight='bold')\n",
    "                plt.legend(loc='upper right')\n",
    "                plt.savefig(\"instance-\"+str(dataset+1).zfill(2)+\"-\"+str(traintype+1).zfill(2)+\".pdf\", format=\"pdf\", dpi=None, facecolor=\"w\", edgecolor=\"w\", orientation=\"portrait\", transparent=True, bbox_inches=\"tight\", pad_inches=0.02, metadata=None)\n",
    "                plt.show()\n",
    "                plt.close()\n",
    "           \n",
    "            if (traindimension == 3):\n",
    "                scatter_x = xready[:,0]\n",
    "                scatter_y = xready[:,1]\n",
    "                scatter_z = xready[:,2]\n",
    "                scatter_x_min = 0\n",
    "                scatter_x_max = 1\n",
    "                scatter_y_min = 0\n",
    "                scatter_y_max = 1\n",
    "                scatter_z_min = 0\n",
    "                scatter_z_max = 1\n",
    "                group = tready\n",
    "                plt.figure(figsize=(8, 6))\n",
    "                for label in np.unique(tready):\n",
    "                    mask = tready == label\n",
    "                    plt.scatter(scatter_x[mask], scatter_y[mask], scatter_z[mask],\n",
    "                                color=colors[label], label=f'Class {label}', s=1, alpha=0.7)\n",
    "                plt.xlabel('X')\n",
    "                plt.ylabel('Y')\n",
    "                plt.zlabel('Z')\n",
    "                plt.xlim([scatter_x_min,scatter_x_max])\n",
    "                plt.ylim([scatter_y_min,scatter_y_max])\n",
    "                plt.zlim([scatter_z_min,scatter_z_max])\n",
    "                plt.title(label_traintype[traintype],fontsize=16,fontweight='bold')\n",
    "                plt.savefig(\"instance-\"+str(dataset+1).zfill(2)+\"-\"+str(traintype+1).zfill(2)+\".pdf\", format=\"pdf\", dpi=None, facecolor=\"w\", edgecolor=\"w\", orientation=\"portrait\", transparent=True, bbox_inches=\"tight\", pad_inches=0.02, metadata=None)\n",
    "                plt.show()\n",
    "                plt.close()\n",
    "\n",
    "        # -------------------------------------------------------------------------------------\n",
    "        # LAZY LEARNING ALGORITHMS\n",
    "        # -------------------------------------------------------------------------------------\n",
    "\n",
    "        # -------------------------------------------------------------------------------------\n",
    "        # Record performance\n",
    "        # -------------------------------------------------------------------------------------\n",
    "\n",
    "        # Execute loop only once for non-knn\n",
    "        if (model != 'knn'): max_k = 0\n",
    "\n",
    "        for k in range(0,max_k,1):\n",
    "\n",
    "            # Training Cross validation\n",
    "            tracemalloc.start()\n",
    "            time_start = time.perf_counter()\n",
    "            if (model == 'knn'):\n",
    "                clf = KNeighborsClassifier(n_neighbors=(2*k)+1,n_jobs=-1)\n",
    "            elif (model == 'svm'):\n",
    "                clf = svm.SVC(kernel='linear', C=1, random_state=0,probability=True)\n",
    "            elif (model == 'nn'):  # Neural Network Model\n",
    "                clf = Sequential([\n",
    "                    Dense(64, activation='relu', input_shape=(input_dim,)),\n",
    "                    Dense(32, activation='relu'),\n",
    "                    Dense(num_classes, activation='softmax')\n",
    "                ])\n",
    "                clf.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "            if min(counts_tready) >= 2:\n",
    "                cross_validation_fold_final = min(cross_validation_fold, min(counts_tready))\n",
    "                cross_validation_error = 0\n",
    "            else:\n",
    "                print(f\"Skipping traintype={traintype} due to insufficient samples (min={min(counts_tready)})\")\n",
    "                cross_validation_error = 1\n",
    "                continue\n",
    "\n",
    "                \n",
    "    \n",
    "            # Custom scorer for Cohen's Kappa\n",
    "            kappa_scorer = make_scorer(cohen_kappa_score)\n",
    "\n",
    "            scoring = {\"accuracy\":\"accuracy\",\n",
    "                       \"precision\":\"precision_weighted\",\n",
    "                       \"recall\":\"recall_weighted\",\n",
    "                       \"f1\":\"f1_weighted\",\n",
    "                       \"aucovr\":\"roc_auc_ovr_weighted\",\n",
    "                       \"aucovo\":\"roc_auc_ovo_weighted\",\n",
    "                       \"kappa\": kappa_scorer}\n",
    "\n",
    "            skf = StratifiedKFold(n_splits=cross_validation_fold_final, random_state=0, shuffle=True)\n",
    "            scores = cross_validate(clf, xready, tready, scoring=scoring, cv=skf, return_train_score=True, n_jobs=-1)\n",
    "\n",
    "            time_stop = time.perf_counter()\n",
    "            current, peak = tracemalloc.get_traced_memory()\n",
    "            tracemalloc.stop()\n",
    "            mem_training_current[traintype,k] = current\n",
    "            mem_training_peak[traintype,k] = peak\n",
    "            time_training[traintype,k] = time_stop - time_start\n",
    "\n",
    "            if (cross_validation_error == 0):\n",
    "                perf_fit_time[traintype,k] = np.nanmean(scores[\"fit_time\"])\n",
    "                perf_score_time[traintype,k] = np.nanmean(scores[\"score_time\"])\n",
    "                perf_train_accuracy[traintype,k] = np.nanmean(scores[\"train_accuracy\"])\n",
    "                perf_train_precision[traintype,k] = np.nanmean(scores[\"train_precision\"])\n",
    "                perf_train_recall[traintype,k] = np.nanmean(scores[\"train_recall\"])\n",
    "                perf_train_f1[traintype,k] = np.nanmean(scores[\"train_f1\"])\n",
    "                perf_train_aucovr[traintype,k] = np.nanmean(scores[\"train_aucovr\"])\n",
    "                perf_train_aucovo[traintype,k] = np.nanmean(scores[\"train_aucovo\"])\n",
    "                perf_train_kappa[traintype,k] = np.nanmean(scores[\"train_kappa\"])\n",
    "            else:\n",
    "                perf_fit_time[traintype,k] = np.nan\n",
    "                perf_score_time[traintype,k] = np.nan\n",
    "                perf_train_accuracy[traintype,k] = np.nan\n",
    "                perf_train_precision[traintype,k] = np.nan\n",
    "                perf_train_recall[traintype,k] = np.nan\n",
    "                perf_train_f1[traintype,k] = np.nan\n",
    "                perf_train_aucovr[traintype,k] = np.nan\n",
    "                perf_train_aucovo[traintype,k] = np.nan\n",
    "                perf_train_kappa[traintype,k] = np.nan\n",
    "\n",
    "            if (model == 'knn'):\n",
    "                print(\"Training: D=\", dataset,\n",
    "                      \"; Type=\",label_traintype[traintype],\n",
    "                      \"; cv=\",cross_validation_fold_final,\n",
    "                      \"; d=\",d,\n",
    "                      \"; k=\",(2*k)+1,\n",
    "                      \"; Train=\",reduction[traintype],\n",
    "                      \"; Acc=\",\"{:0.2f}\".format(perf_train_accuracy[traintype,k]),\n",
    "                      \"; Pre=\",\"{:0.2f}\".format(perf_train_precision[traintype,k]),\n",
    "                      \"; Recall=\",\"{:0.2f}\".format(perf_train_recall[traintype,k]),\n",
    "                      \"; F1=\",\"{:0.2f}\".format(perf_train_f1[traintype,k]),\n",
    "                      \"; AUC OVR=\",\"{:0.2f}\".format(perf_train_aucovr[traintype,k]),\n",
    "                      \"; AUC OVO=\",\"{:0.2f}\".format(perf_train_aucovo[traintype,k]),sep = \"\")\n",
    "            elif (model == 'svm2'):\n",
    "                print(\"Training: D=\", dataset,\n",
    "                      \"; Type=\",label_traintype[traintype],\n",
    "                      \"; cv=\",cross_validation_fold_final,\n",
    "                      \"; d=\",d,\n",
    "                      \"; Train=\",reduction[traintype],\n",
    "                      \"; Acc=\",\"{:0.2f}\".format(sum(perf_train_accuracy[traintype])/len(perf_train_accuracy[traintype])),\n",
    "                      \"; Pre=\",\"{:0.2f}\".format(sum(perf_train_precision[traintype]/len(perf_train_precision[traintype]))),\n",
    "                      \"; Recall=\",\"{:0.2f}\".format(sum(perf_train_recall[traintype]/len(perf_train_recall[traintype]))),\n",
    "                      \"; F1=\",\"{:0.2f}\".format(sum(perf_train_f1[traintype]/len(perf_train_f1[traintype]))),\n",
    "                      \"; AUC OVR=\",\"{:0.2f}\".format(sum(perf_train_aucovr[traintype]/len(perf_train_aucovr[traintype]))),\n",
    "                      \"; AUC OVO=\",\"{:0.2f}\".format(sum(perf_train_aucovo[traintype]/len(perf_train_aucovo[traintype]))), sep = \"\")\n",
    "\n",
    "        # -------------------------------------------------------------------------------------\n",
    "        # Save training performance and execution time\n",
    "        # -------------------------------------------------------------------------------------\n",
    "\n",
    "        if (performance_save == 1):\n",
    "            with open(\"cross_fit_time.txt\", \"ab\") as file:\n",
    "                np.savetxt(file, perf_fit_time[traintype].reshape(1,max_k), delimiter=\"\\t\", fmt=\"%1.8f\")\n",
    "            with open(\"cross_score_time.txt\", \"ab\") as file:\n",
    "                np.savetxt(file, perf_score_time[traintype].reshape(1,max_k), delimiter=\"\\t\", fmt=\"%1.8f\")\n",
    "            with open(\"cross_train_accuracy.txt\", \"ab\") as file:\n",
    "                np.savetxt(file, perf_train_accuracy[traintype].reshape(1,max_k), delimiter=\"\\t\", fmt=\"%1.4f\")\n",
    "            with open(\"cross_train_precision.txt\", \"ab\") as file:\n",
    "                np.savetxt(file, perf_train_precision[traintype].reshape(1,max_k), delimiter=\"\\t\", fmt=\"%1.4f\")        \n",
    "            with open(\"cross_train_recall.txt\", \"ab\") as file:\n",
    "                np.savetxt(file, perf_train_recall[traintype].reshape(1,max_k), delimiter=\"\\t\", fmt=\"%1.4f\")        \n",
    "            with open(\"cross_train_f1.txt\", \"ab\") as file:\n",
    "                np.savetxt(file, perf_train_f1[traintype].reshape(1,max_k), delimiter=\"\\t\", fmt=\"%1.4f\")        \n",
    "            with open(\"cross_train_aucovr.txt\", \"ab\") as file:\n",
    "                np.savetxt(file, perf_train_aucovr[traintype].reshape(1,max_k), delimiter=\"\\t\", fmt=\"%1.4f\")        \n",
    "            with open(\"cross_train_aucovo.txt\", \"ab\") as file:\n",
    "                np.savetxt(file, perf_train_aucovo[traintype].reshape(1,max_k), delimiter=\"\\t\", fmt=\"%1.4f\")        \n",
    "            with open(\"cross_train_kappa.txt\", \"ab\") as file:\n",
    "                np.savetxt(file, perf_train_kappa[traintype].reshape(1,max_k), delimiter=\"\\t\", fmt=\"%1.4f\")        \n",
    "            with open(\"training_time.txt\", \"ab\") as file:\n",
    "                np.savetxt(file, time_training[traintype].reshape(1,max_k), delimiter=\"\\t\", fmt=\"%1.4f\")        \n",
    "            with open(\"training_mem_current.txt\", \"ab\") as file:\n",
    "                np.savetxt(file, mem_training_current[traintype].reshape(1,max_k), delimiter=\"\\t\", fmt=\"%0.0f\")        \n",
    "            with open(\"training_mem_peak.txt\", \"ab\") as file:\n",
    "                np.savetxt(file, mem_training_peak[traintype].reshape(1,max_k), delimiter=\"\\t\", fmt=\"%0.0f\")        \n",
    "            print(\"Saving: Set=\",dataset,\"; type=\",traintype,sep = \"\")\n",
    "                  \n",
    "        # Print average scores\n",
    "        if (model == 'knn'):\n",
    "            print(\"Training: D=\", dataset,\n",
    "                  \"; Type=\",label_traintype[traintype],\n",
    "                  \"; cv=\",cross_validation_fold_final,\n",
    "                  \"; d=\",d,\n",
    "                  \"; k=average\",\n",
    "                  \"; Train=\",reduction[traintype],\n",
    "                  \"; Acc=\",\"{:0.2f}\".format(np.nanmean(perf_train_accuracy[traintype],axis=0)),\n",
    "                  \"; Pre=\",\"{:0.2f}\".format(np.nanmean(perf_train_precision[traintype],axis=0)),\n",
    "                  \"; Recall=\",\"{:0.2f}\".format(np.nanmean(perf_train_recall[traintype],axis=0)),\n",
    "                  \"; F1=\",\"{:0.2f}\".format(np.nanmean(perf_train_f1[traintype],axis=0)),\n",
    "                  \"; AUC OVR=\",\"{:0.2f}\".format(np.nanmean(perf_train_aucovr[traintype],axis=0)),\n",
    "                  \"; AUC OVO=\",\"{:0.2f}\".format(np.nanmean(perf_train_aucovo[traintype],axis=0)),sep = \"\")\n",
    "        elif (model == 'svm'):\n",
    "            print(\"Training: D=\", dataset,\n",
    "                  \"; Type=\",label_traintype[traintype],\n",
    "                  \"; cv=\",cross_validation_fold_final,\n",
    "                  \"; d=\",d,\n",
    "                  \"; Train=\",reduction[traintype],\n",
    "                  \"; Acc=\",\"{:0.2f}\".format(np.nanmean(perf_train_accuracy[traintype],axis=0)),\n",
    "                  \"; Pre=\",\"{:0.2f}\".format(np.nanmean(perf_train_precision[traintype],axis=0)),\n",
    "                  \"; Recall=\",\"{:0.2f}\".format(np.nanmean(perf_train_recall[traintype],axis=0)),\n",
    "                  \"; F1=\",\"{:0.2f}\".format(np.nanmean(perf_train_f1[traintype],axis=0)),\n",
    "                  \"; AUC OVR=\",\"{:0.2f}\".format(np.nanmean(perf_train_aucovr[traintype],axis=0)),\n",
    "                  \"; AUC OVO=\",\"{:0.2f}\".format(np.nanmean(perf_train_aucovo[traintype],axis=0)),sep = \"\")\n",
    "        print()\n",
    "\n",
    "    # -------------------------------------------------------------------------------------\n",
    "    # Save data reduction performance\n",
    "    # -------------------------------------------------------------------------------------\n",
    "\n",
    "    if (performance_save == 1):\n",
    "        with open(\"class-balance.txt\", \"ab\") as file:\n",
    "            np.savetxt(file, counts_train.reshape(1,counts_train.shape[0]), delimiter=\"\\t\", fmt=\"%0.0f\")\n",
    "        with open(\"class-balance_percentage.txt\", \"ab\") as file:\n",
    "            np.savetxt(file, (counts_train/traindata.shape[0]).reshape(1,counts_train.shape[0]), delimiter=\"\\t\", fmt=\"%0.4f\")\n",
    "        with open(\"reduction-size.txt\", \"ab\") as file:\n",
    "            np.savetxt(file, reduction.reshape(1,reduction.shape[0]), delimiter=\"\\t\", fmt=\"%0.0f\")\n",
    "        with open(\"reduction-class.txt\", \"ab\") as file:\n",
    "            np.savetxt(file, np.transpose(reduction_class), delimiter=\"\\t\", fmt=\"%0.0f\")\n",
    "        with open(\"reduction-time.txt\", \"ab\") as file:\n",
    "            np.savetxt(file, time_reduction.reshape(1,max_type), delimiter=\"\\t\", fmt=\"%1.4f\")        \n",
    "\n",
    "# -------------------------------------------------------------------------------------\n",
    "# Save dataset-level data reduction performance\n",
    "# -------------------------------------------------------------------------------------\n",
    "\n",
    "outputfile = \"reduction_kd_query_miss.txt\"\n",
    "with open(outputfile, \"ab\") as file:\n",
    "    np.savetxt(file, kd_query_miss, delimiter=\"\\t\", fmt=\"%0.0f\")  \n",
    "\n",
    "print(\"Evaluation: Dataset\",dataset,\"is done!\")\n",
    "print(\"------------------------------------------------------------------------------\\n\\r\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
